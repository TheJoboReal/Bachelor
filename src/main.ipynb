{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys          # We use sys to get the max value of a float\n",
    "import pandas as pd # We only use pandas for displaying tables nicely\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World class and globals\n",
    "\n",
    "This is how we generate a world and the global variables we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": -1, \".\": 2, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:\n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    # Create an empty world where the agent can move to all cells\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "\n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "\n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES\n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\"\n",
    "    Return the reward associated with a given location\n",
    "    \"\"\"\n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of\n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"\n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "    x, y = current_state\n",
    "\n",
    "    # If our current state is a terminal, there is no next state\n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    # Check of a random action should be performed:\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    # If the next state is an obstacle, stay in the current state\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' '+' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0               \n",
       "1               \n",
       "2               \n",
       "3        +      \n",
       "4               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = World(5,5)\n",
    "\n",
    "world.add_reward(2,3, \"+\")\n",
    "\n",
    "def equiprobable_random_policy(x, y):\n",
    "  return { k:1/len(ACTIONS) for k in ACTIONS }\n",
    "\n",
    "print(world.grid.T)\n",
    "display(pd.DataFrame(world.grid.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'World' object has no attribute 'get_state_transition_probabilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[38;5;66;03m# Return the state value estimates\u001b[39;00m\n\u001b[1;32m     43\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m V\n\u001b[0;32m---> 46\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[43miterative_policy_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequiprobable_random_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m display(pd\u001b[38;5;241m.\u001b[39mDataFrame(V\u001b[38;5;241m.\u001b[39mT))\n",
      "Cell \u001b[0;32mIn[41], line 27\u001b[0m, in \u001b[0;36miterative_policy_evaluation\u001b[0;34m(world, policy, theta, max_iterations)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# loop over all actions that our policy says that we can perform\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# in the current state:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action, action_prob \u001b[38;5;129;01min\u001b[39;00m actions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;66;03m# For each action, get state transition probabilities and\u001b[39;00m\n\u001b[1;32m     25\u001b[0m   \u001b[38;5;66;03m# accumulate in v rewards weighted with action and state transition\u001b[39;00m\n\u001b[1;32m     26\u001b[0m   \u001b[38;5;66;03m# probabilities:\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m (xi, yi), state_prob \u001b[38;5;129;01min\u001b[39;00m \u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_transition_probabilities\u001b[49m((x, y), action)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     28\u001b[0m     v \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m action_prob \u001b[38;5;241m*\u001b[39m state_prob \u001b[38;5;241m*\u001b[39m (world\u001b[38;5;241m.\u001b[39mget_reward(xi, yi) \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m V[xi, yi])\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# update delta (largest change in estimate so far)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'World' object has no attribute 'get_state_transition_probabilities'"
     ]
    }
   ],
   "source": [
    "def iterative_policy_evaluation(world, policy, theta=1e-5, max_iterations=1e3):\n",
    "\n",
    "  # Our initial estimates for all states in the world is 0:\n",
    "  V = np.full((world.width, world.height), 0.0)\n",
    "\n",
    "  while True:\n",
    "    # delta keeps track of the largest change in one iteration, so we set it to\n",
    "    # 0 at the start of each iteration:\n",
    "    delta = 0\n",
    "\n",
    "    # Loop over all states (x,y)\n",
    "    for y in range(world.height):\n",
    "      for x in range(world.width):\n",
    "        if not world.is_obstacle(x, y):\n",
    "          # Get action probabilities for the current state:\n",
    "          actions = policy(x, y)\n",
    "\n",
    "          # v is the new estimate that will be updated in the loop:\n",
    "          v = 0\n",
    "\n",
    "          # loop over all actions that our policy says that we can perform\n",
    "          # in the current state:\n",
    "          for action, action_prob in actions.items():\n",
    "            # For each action, get state transition probabilities and\n",
    "            # accumulate in v rewards weighted with action and state transition\n",
    "            # probabilities:\n",
    "            for (xi, yi), state_prob in world.get_state_transition_probabilities((x, y), action).items():\n",
    "              v += action_prob * state_prob * (world.get_reward(xi, yi) + gamma * V[xi, yi])\n",
    "\n",
    "          # update delta (largest change in estimate so far)\n",
    "          delta = max(delta, abs(v - V[x, y]))\n",
    "          V[x, y] = v\n",
    "\n",
    "    # check if current state value estimates are close enought to end:\n",
    "    if delta <= theta:\n",
    "      break\n",
    "\n",
    "    max_iterations -= 1\n",
    "    if max_iterations == 0:\n",
    "      break\n",
    "\n",
    "  # Return the state value estimates\n",
    "  return V\n",
    "\n",
    "\n",
    "V = iterative_policy_evaluation(world, equiprobable_random_policy)\n",
    "\n",
    "display(pd.DataFrame(V.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

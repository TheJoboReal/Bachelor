{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMapQn0-y2Bk"
   },
   "source": [
    "# Q-learning\n",
    "\n",
    "In this notebook, you will implement Q-learning as described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9vApPmQFGUs"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9pkLaPNMOTP",
    "outputId": "5f0bfd91-826a-41cc-f06a-c4be06d87d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/kasper/Documents/Civilingenioer_Robotteknologi/Bachelor/.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: pandas in /home/kasper/Documents/Civilingenioer_Robotteknologi/Bachelor/.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kasper/Documents/Civilingenioer_Robotteknologi/Bachelor/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kasper/Documents/Civilingenioer_Robotteknologi/Bachelor/.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kasper/Documents/Civilingenioer_Robotteknologi/Bachelor/.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/kasper/Documents/Civilingenioer_Robotteknologi/Bachelor/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys          # We use sys to get the max value of a float\n",
    "import pandas as pd # We only use pandas for displaying tables nicely\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTNglEH9FR8f"
   },
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\") \n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = (\"#\")\n",
    "WORLD_SIZE = 10\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "path = []\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:  \n",
    "\tdef __init__(self, width, height):\n",
    "\t\tself.width = width\n",
    "\t\tself.height = height\n",
    "\t\t# Create an empty world where the agent can move to all cells\n",
    "\t\tself.grid = np.full((width, height), ' ', dtype='U1')\n",
    "\t\n",
    "\tdef add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "\t\t\"\"\"\n",
    "\t\tCreate an obstacle in either a single cell or rectangle.\n",
    "\t\t\"\"\"\n",
    "\t\tif end_x == None: end_x = start_x\n",
    "\t\tif end_y == None: end_y = start_y\n",
    "\t\t\n",
    "\t\tself.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "\tdef add_reward(self, x, y, reward):\n",
    "\t\tassert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "\t\tself.grid[x, y] = reward\n",
    "\n",
    "\tdef add_terminal(self, x, y, terminal):\n",
    "\t\tassert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "\t\tself.grid[x, y] = terminal\n",
    "\n",
    "\tdef is_obstacle(self, x, y):\n",
    "\t\tif x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\treturn self.grid[x ,y] in OBSTACLES \n",
    "\n",
    "\tdef is_terminal(self, x, y):\n",
    "\t\treturn self.grid[x ,y] in TERMINALS\n",
    "\n",
    "\tdef get_reward(self, x, y):\n",
    "\t\t\"\"\" \n",
    "\t\tReturn the reward associated with a given location\n",
    "\t\t\"\"\" \n",
    "\t\treturn REWARDS[self.grid[x, y]]\n",
    "\n",
    "\tdef get_next_state(self, current_state, action):\n",
    "\t\t\"\"\"\n",
    "\t\tGet the next state given a current state and an action. The outcome can be\n",
    "\t\tstochastic  where rand_move_probability determines the probability of \n",
    "\t\tignoring the action and performing a random move.\n",
    "\t\t\"\"\"    \n",
    "\t\tassert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\t\t\n",
    "\t\tx, y = current_state \n",
    "\t\t\n",
    "\t\t# If our current state is a terminal, there is no next state\n",
    "\t\tif self.grid[x, y] in TERMINALS:\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\t# Check of a random action should be performed:\n",
    "\t\tif np.random.rand() < rand_move_probability:\n",
    "\t\t\taction = np.random.choice(ACTIONS)\n",
    "\n",
    "\t\tif action == \"up\":      y -= 1\n",
    "\t\telif action == \"down\":  y += 1\n",
    "\t\telif action == \"left\":  x -= 1\n",
    "\t\telif action == \"right\": x += 1\n",
    "\n",
    "\t\t# If the next state is an obstacle, stay in the current state\n",
    "\t\treturn (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKybFbwCG478"
   },
   "source": [
    "## A simple world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVpq3vosHAol",
    "outputId": "4045b463-0521-4116-a8ea-067972fa57bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' '#' '#' '#' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' '.' '#' '#' ' ' '.' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' '#' '#' '#' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' '.' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '.' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' '.' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '.' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '\n",
      "  ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "world = World(WORLD_SIZE, WORLD_SIZE)\n",
    "\n",
    "# Add obstacles\n",
    "world.add_obstacle(1, 1, 3, 3)\n",
    "world.add_reward(2, 6, \".\")\n",
    "world.add_reward(5, 2, \".\")\n",
    "world.add_reward(2, 8, \".\")\n",
    "world.add_reward(1, 2, \".\")\n",
    "world.add_reward(9, 9, \".\")\n",
    "world.add_reward(8, 7, \".\")\n",
    "world.add_reward(2, 6, \".\")\n",
    "\n",
    "\n",
    "# Note, that we have to transpose the 2D array (.T) for (x,y)\n",
    "# to match the convention when displayed\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK_bFSHsQJYr"
   },
   "source": [
    "## Exercise: Q-learning\n",
    "\n",
    "Implement and test Q-learning. You should be able to base much of your code on your implementation of SARSA. Since Q-learning is an off-policy method, we can use whatever behavior policy we want during training, but the choice of behavioral policy still manners so it is a good idea to balance exploration and exploitation. During testing, we can then use the learnt policy (the target policy).\n",
    "\n",
    "As for the behavior policy, you can use an simple $\\epsilon$-greedy policy, but you can also experiment with alternatives, for instance, optimistic initial values. \n",
    "\n",
    "See page 131 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the Q-learning algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEIxyiw1J1fd"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m step_list_q_lern \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m     63\u001b[0m \t\u001b[38;5;66;03m# print(f\"Episode {episode + 1 }/{EPISODES}:\")\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \tsteps \u001b[38;5;241m=\u001b[39m \u001b[43mq_learn_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_greedy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraand_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \tstep_list_q_lern\u001b[38;5;241m.\u001b[39mappend(steps)\n\u001b[1;32m     68\u001b[0m Q_best \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((world\u001b[38;5;241m.\u001b[39mwidth,world\u001b[38;5;241m.\u001b[39mheight),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m               \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m, in \u001b[0;36mq_learn_episode\u001b[0;34m(world, policy, start_state)\u001b[0m\n\u001b[1;32m     34\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Chose A from S using policy derived from Q\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Take Action A observe R, S'\u001b[39;00m\n\u001b[1;32m     40\u001b[0m next_state \u001b[38;5;241m=\u001b[39m world\u001b[38;5;241m.\u001b[39mget_next_state(current_state, ACTIONS[A])\n",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m, in \u001b[0;36mepsilon_greedy\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mepsilon_greedy\u001b[39m(x,y):\n\u001b[0;32m---> 23\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m epsilon:\n\u001b[1;32m     24\u001b[0m \t\t\u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;28mlen\u001b[39m(ACTIONS))\n\u001b[1;32m     25\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: Implement your code here -- you need a Q-table to keep track of action \n",
    "#       value estimates and a policy-function that returns an epsilon greedy \n",
    "#       policy based on your estimates. \n",
    "\n",
    "# Global variable to keep track of current estimates\n",
    "# print(len(ACTIONS))\n",
    "Q = np.full((world.width,world.height,4),0.0)\n",
    "# print(\"org:\")\n",
    "# print(Q)\n",
    "\n",
    "# Our step size / learing rate \n",
    "alpha = 0.05 \n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "epsilon = 0.5\n",
    "\n",
    "# Episodes to run \n",
    "EPISODES = 1000\n",
    "\n",
    "def epsilon_greedy(x,y):\n",
    "\tif np.random.rand() < epsilon:\n",
    "\t\treturn random.randrange(len(ACTIONS))\n",
    "\telse:\n",
    "\t\treturn np.argmax(Q[x,y,:])\n",
    "\n",
    "def q_learn_episode(world, policy, start_state):\n",
    "\t# Initialise S:\n",
    "\tcurrent_state = start_state\n",
    "\tsteps = 0\n",
    "\t# Loop for each step of episode\n",
    "\twhile not world.is_terminal(*current_state):\n",
    "\t\tsteps += 1\n",
    "\t\t\n",
    "\t\t# Chose A from S using policy derived from Q\n",
    "\t\tA = policy(current_state[0],current_state[1])\n",
    "\n",
    "\t\t# Take Action A observe R, S'\n",
    "\t\tnext_state = world.get_next_state(current_state, ACTIONS[A])\n",
    "\t\treward = world.get_reward(*next_state)\n",
    "\n",
    "\t\t# Q(S,A) <- Q(S,A) + alpha(R + gamme * max_a(Q(S',a)-Q(S,A)))\n",
    "\t\tS =         (current_state[0],current_state[1],A)\n",
    "\t\tS_next =    (next_state[0],next_state[1])        \n",
    "\t\tQ[S] = Q[S] + alpha*(reward + gamma*max(Q[S_next])-Q[S]) \n",
    "\t\t\n",
    "\t\t# S <- S'\n",
    "\t\tcurrent_state = next_state\n",
    "\t\n",
    "\treturn steps\n",
    "\t\t\n",
    "def raand_start():\n",
    "\tx = np.random.randint(0,world.width)\n",
    "\ty = np.random.randint(0,world.height)\n",
    "\tif world.is_obstacle(x,y):\n",
    "\t\tx,y = raand_start()\n",
    "\treturn x,y\n",
    "\n",
    "step_list_q_lern = []\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "\t# print(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "\tsteps = q_learn_episode(world, epsilon_greedy, raand_start())\n",
    "\tstep_list_q_lern.append(steps)\n",
    "\n",
    "\n",
    "Q_best = np.full((world.width,world.height),\"               \")\n",
    "# print(\"new:\")\n",
    "# print(Q)\n",
    "\n",
    "for w in range(world.width):\n",
    "\tfor h in range(world.height):\n",
    "\t\tQ_best[w,h] = ACTIONS[np.argmax(Q[w,h,:])]\n",
    "\n",
    "\n",
    "display(pd.DataFrame(Q_best.T))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Q_lern\")\n",
    "plt.plot(step_list_q_lern[:500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMdPGy2dxxRb"
   },
   "source": [
    "## Exercise: Compare Q-learning and SARSA\n",
    "\n",
    "Setup experiments to compare the performance of Q-learning and SARSA. You can use different ```Worlds``` and test different parameter setting, e.g. for $\\alpha$ and $\\epsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7y7ImAjWckA"
   },
   "outputs": [],
   "source": [
    "### TODO: Implement your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHHfshQ8RtP-"
   },
   "source": [
    "## Optional exercise: Maximization Bias and Double Learning\n",
    "\n",
    "Below is an implementation of the task shown in Example 6.7 on page 134 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). There are two states, ```A``` and ```B``` where the agent can perform actions, and a terminal state ```T```. ```A``` and ```B``` have different actions available:\n",
    "\n",
    "* ```A``` has ```left``` (to ```B```) and ```right``` to the terminal state\n",
    "* ```B``` has a larger number of actions all leading to a terminal state. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSPgN1YRI9dx"
   },
   "outputs": [],
   "source": [
    "# States \"A\" and \"B\" have actions while \"T\" is a terminal state.\n",
    "STATES = (\"A\", \"B\", \"T\")\n",
    "\n",
    "class Example67MDP:\n",
    "\tdef __init__(self, number_of_B_actions):\n",
    "\t\t\"\"\"\n",
    "\t\tCreate an example and set the number of outgoing actions for state \"B\"\n",
    "\t\t(in the book, they do not give a specific number, but merely write that \n",
    "\t\tfrom \"B\" there \"are many possible actions all of which cause immediate\n",
    "\t\ttermination with a reward drawn from a normal distribution with mean \n",
    "\t\t-0.1 and variance 1. So, feel free to play with different number of \n",
    "\t\tactions in state B)\n",
    "\t\t\n",
    "\t\t\"\"\" \n",
    "\t\tself.number_of_B_actions = number_of_B_actions\n",
    "\n",
    "\tdef get_actions(self, state):\n",
    "\t\t\"\"\"\n",
    "\t\tReturns the set of actions availabe in a given state (a tuple \n",
    "\t\twith strings).  \n",
    "\t\t\"\"\"\n",
    "\t\tassert state in STATES, f\"State must be one of {STATES}, not {state}\"\n",
    "\t\tif state == \"A\":\n",
    "\t\t\treturn (\"left\", \"right\")\n",
    "\t\tif state == \"B\":\n",
    "\t\t\treturn tuple(f\"{i}\" for i in range(self.number_of_B_actions))\n",
    "\t\tif state == \"T\":\n",
    "\t\t\treturn tuple(\"N\")\n",
    "\n",
    "\tdef get_next_state_and_reward(self, state, action):\n",
    "\t\t\"\"\"\n",
    "\t\tGet the next state and reward given a current state and an action\n",
    "\t\t\"\"\"\n",
    "\t\tassert state in STATES, f\"Unknown state: {state}\"\n",
    "\t\tprint(state)\n",
    "\t\tprint(self.get_actions(state))\n",
    "\t\tassert action in self.get_actions(state), f\"Unknown action {action} for state {state}\"\n",
    "\t\t \n",
    "\t\tif state == \"T\":\n",
    "\t\t\traise Exception(\"The terminal state has no actions and no next state\")\n",
    "\n",
    "\t\tif state == \"A\":\n",
    "\t\t\tif action == \"right\":\n",
    "\t\t\t\treturn \"T\", 0\n",
    "\t\t\tif action == \"left\":\n",
    "\t\t\t\treturn \"B\", 0\n",
    "\n",
    "\t\tif state == \"B\":\n",
    "\t\t\treturn \"T\", np.random.normal(loc = -0.1)\n",
    "\n",
    "\tdef is_terminal(self, state):\n",
    "\t\tassert state in STATES, f\"Unknown state: {state}\"\n",
    "\t\treturn state == \"T\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ippehOfaxbVu"
   },
   "source": [
    "Implement Double Q-learning (see page 136 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)) and test it on the ```Example67MDP``` above. Notice, that the number of actions differs between the two states ($\\mathcal{A}$(```\"A\"```) $\\neq \\mathcal{A}$(```\"B\"```)), which you have to take into account in your Q-tables. See the code for ```Example67MDP``` above: you can get the set of actions available in a given state by calling ```get_actions(...)``` with the state as argument.\n",
    "\n",
    "Compare action-value estimates for ```\"left\"``` and ```\"right\"``` in state ```\"A\"```  at different times during learning when using double-Q learning and when using normal Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2YL07t5yiUe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000:\n",
      "A\n",
      "A\n",
      "A\n",
      "('left', 'right')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Unknown action {'left': 0.9500000000000001, 'right': 0.05} for state A",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[1;32m     61\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPISODES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \tsteps \u001b[38;5;241m=\u001b[39m \u001b[43mdouble_q_learn_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_greedy_dql_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \tstep_list_q_lern\u001b[38;5;241m.\u001b[39mappend(steps)\n\u001b[1;32m     66\u001b[0m Q1_best \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((world\u001b[38;5;241m.\u001b[39mwidth,world\u001b[38;5;241m.\u001b[39mheight),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m               \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mdouble_q_learn_episode\u001b[0;34m(world, policy, start_state)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(current_state)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Take Action A observe R, S'\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m state_next, reward \u001b[38;5;241m=\u001b[39m \u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_state_and_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Q(S,A) <- Q(S,A) + alpha(R + gamme * max_a(Q(S',a)-Q(S,A)))\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m):     \n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mExample67MDP.get_next_state_and_reward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_actions(state))\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_actions(state), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown action \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     40\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe terminal state has no actions and no next state\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Unknown action {'left': 0.9500000000000001, 'right': 0.05} for state A"
     ]
    }
   ],
   "source": [
    "# Create an instance of Example 6.7 with 10 actions in B\n",
    "example = Example67MDP(10)\n",
    "\n",
    "gamma = 1\n",
    "alpha = 0.05\n",
    "epsilon = 0.1\n",
    "\n",
    "# Create two Q-tables (feel free to use your own representation):\n",
    "# Q1 = np.full((world.width,world.height,3),0.0)\n",
    "# Q2 = np.full((world.width,world.height,3),0.0)\n",
    "Q1 = [[0 for _ in range(len(example.get_actions(state)))] for state in STATES]\n",
    "Q2 = [[0 for _ in range(len(example.get_actions(state)))] for state in STATES]\n",
    "\n",
    "# Uncomment to disable double-Q-learning:\n",
    "#Q2 = Q1\n",
    "\n",
    "# You can use the below policy method if you use the Q1 and Q2 as defined above. \n",
    "# If you have done your own representation, you probably have to modify or \n",
    "# rewrite the function below:\n",
    "\n",
    "def e_greedy_dql_policy(state):\n",
    "  global example\n",
    "  print(state)\n",
    "  actions = { a:epsilon/len(example.get_actions(state)) for a in example.get_actions(state) }\n",
    "  # Do a Q1 + Q2 to do epsilon greedy based on both tables:\n",
    "  Q = [sum(x) for x in zip(Q1[STATES.index(state)], Q2[STATES.index(state)])]\n",
    "  actions[example.get_actions(state)[np.argmax(Q)]] = 1 - epsilon + epsilon/len(example.get_actions(state))\n",
    "  return actions\n",
    "\n",
    "### TODO: Implement double Q-learning\n",
    "def double_q_learn_episode(world, policy, start_state):\n",
    "\t# Initialise S:\n",
    "\tcurrent_state = start_state\n",
    "\t\n",
    "\tsteps = 0\n",
    "\t\n",
    "\t# Loop for each step of episode\n",
    "\twhile not world.is_terminal(*current_state):\n",
    "\t\tsteps += 1\n",
    "\t\t\n",
    "\t\t# Chose A from S using policy derived from Q1 +  Q2\n",
    "\t\tA = policy(current_state)\n",
    "\t\tprint(current_state)\n",
    "\n",
    "\t\t# Take Action A observe R, S'\n",
    "\t\tstate_next, reward = world.get_next_state_and_reward(current_state,A)\n",
    "\n",
    "\t\t# Q(S,A) <- Q(S,A) + alpha(R + gamme * max_a(Q(S',a)-Q(S,A)))\n",
    "\n",
    "\t\tif(np.random.rand() > 0.5):     \n",
    "\t\t\tQ1[current_state] = Q1[current_state] + alpha*(reward + gamma*Q2[state_next,np.argmax(Q1[state_next,A])]-Q1[current_state])\n",
    "\t\telse:\n",
    "\t\t\tQ2[current_state] = Q2[current_state] + alpha*(reward + gamma*Q1[state_next,np.argmax(Q2[state_next,A])]-Q2[current_state])\n",
    "\t\t\t\n",
    "\t\t# S <- S'\n",
    "\t\tcurrent_state = state_next\n",
    "\t\n",
    "\treturn steps\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "\tprint(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "\tsteps = double_q_learn_episode(example, e_greedy_dql_policy, \"A\")\n",
    "\tstep_list_q_lern.append(steps)\n",
    "\n",
    "\n",
    "Q1_best = np.full((world.width,world.height),\"               \")\n",
    "# print(\"new:\")\n",
    "# print(Q)\n",
    "\n",
    "for w in range(world.width):\n",
    "\tfor h in range(world.height):\n",
    "\t\tQ1_best[w,h] = ACTIONS[np.argmax(Q1[w,h,:])]\n",
    "\n",
    "\n",
    "display(pd.DataFrame(Q1_best.T))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title(\"Q_lern\")\n",
    "plt.plot(step_list_q_lern[:100])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMapQn0-y2Bk"
   },
   "source": [
    "# Policy iteration and value iteration\n",
    "\n",
    "In this notebook, you will implement different dynamic programming approaches described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). A grid ```World``` class and policy iteration has been implemented. Feel free to add more actions, rewards and/or terminals, or to modify the code to suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9vApPmQFGUs"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9pkLaPNMOTP",
    "outputId": "8adb5191-7368-4ed0-ecf0-730f2fe217ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/vaede/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: pandas in /home/vaede/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vaede/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/vaede/.local/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyQt6 in /home/vaede/.local/lib/python3.10/site-packages (6.8.1)\n",
      "Requirement already satisfied: PyQt6-sip<14,>=13.8 in /home/vaede/.local/lib/python3.10/site-packages (from PyQt6) (13.10.0)\n",
      "Requirement already satisfied: PyQt6-Qt6<6.9.0,>=6.8.0 in /home/vaede/.local/lib/python3.10/site-packages (from PyQt6) (6.8.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas\n",
    "\n",
    "! pip install PyQt6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8kdEPNmFOCr"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wl4F2OZK3Xz3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "T9cAvA0GLkXh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys          # We use sys to get the max value of a float\n",
    "import pandas as pd # We only use pandas for displaying tables nicely\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTNglEH9FR8f"
   },
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called. It will only be relevant to call this function later on, when we do learning based on interaction with the environment and where an agent actually has to move.\n",
    "\n",
    "For now, you will only need the probabilities over next states given an action, $a$, that is, call ```get_state_transition_probabilities```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfR2S8j_LnZ3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMAd6qTASn9u"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "#Tempreture parameter (for Boltzmann policy) [high beta is greedy low is exploration]\n",
    "beta = 0.1\n",
    "\n",
    "#numbers of drones to simulate\n",
    "number_of_agents = 10\n",
    "\n",
    "\n",
    "class World:\n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    # Create an empty world where the agent can move to all cells\n",
    "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
    "\n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "\n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def add_terminal(self, x, y, terminal):\n",
    "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "    self.grid[x, y] = terminal\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES\n",
    "\n",
    "  def is_terminal(self, x, y):\n",
    "    return self.grid[x ,y] in TERMINALS\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\"\n",
    "    Return the reward associated with a given location\n",
    "    \"\"\"\n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action, deterministic=False):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. Can eiter be\n",
    "    deterministic (no random actions) or non-deterministic,\n",
    "    where rand_move_probability determines the probability of ignoring the\n",
    "    action and performing a random move.\n",
    "    \"\"\"\n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "    x, y = current_state\n",
    "\n",
    "    # If our current state is a terminal, there is no next state\n",
    "    if self.grid[x, y] in TERMINALS:\n",
    "      return None\n",
    "\n",
    "    # Check of a random action should be performed:\n",
    "    if not deterministic and np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    # If the next state is an obstacle, stay in the current state\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n",
    "\n",
    "  def get_state_transition_probabilities(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Returns a dict where key = state and value = probability given current state\n",
    "    is (x,y) and \"action\" is performed.\n",
    "    \"\"\"\n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "    x, y = current_state\n",
    "    if self.is_terminal(x, y):\n",
    "      return {}\n",
    "\n",
    "    next_state_probabilities = {}\n",
    "    # Since there is rand_move_probability of performing any action, we have to\n",
    "    # go through all actions and check what their next state would be:\n",
    "    for a in ACTIONS:\n",
    "      next_state = self.get_next_state((x, y), a, deterministic=True)\n",
    "      if a == action:\n",
    "        prob = 1 - rand_move_probability + rand_move_probability / len(ACTIONS)\n",
    "      else:\n",
    "        prob = rand_move_probability / len(ACTIONS)\n",
    "\n",
    "      if next_state in next_state_probabilities:\n",
    "        next_state_probabilities[next_state] += prob\n",
    "      else:\n",
    "        if prob > 0.0:\n",
    "          next_state_probabilities[next_state] = prob\n",
    "\n",
    "    return next_state_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Drone:\n",
    "    def __init__(self, drone_id, world, start_position=None, q_table_file=\"q_table.npy\"):\n",
    "        self.id = drone_id\n",
    "        self.world = world\n",
    "        self.q_table_file = q_table_file\n",
    "        self.Q = self.load_or_initialize_q_table()\n",
    "        self.position = start_position if start_position else (np.random.randint(0, world.width), np.random.randint(0, world.height))\n",
    "        \n",
    "        # Ensure starting position is not an obstacle\n",
    "        while self.world.is_obstacle(*self.position):\n",
    "            self.position = (np.random.randint(0, world.width), np.random.randint(0, world.height))\n",
    "        \n",
    "        self.cumulative_reward = 0  # Track total collected rewards\n",
    "        self.visited_positions = [self.position]  # Track visited positions\n",
    "        self.active = True  # Drone status\n",
    "    \n",
    "    def compute_reward(self, position):\n",
    "        \"\"\" Compute the reward for moving to a new position \"\"\"\n",
    "        for drone in number_of_agents:\n",
    "            \n",
    "        return self.world.get_reward(*position)\n",
    "    \n",
    "    def softmax_policy(self, available_actions):\n",
    "        \"\"\" Select an action using softmax over rewards \"\"\"\n",
    "        q_values = np.array([self.compute_reward(pos) for pos in available_actions])\n",
    "        exp_q = np.exp(q_values)\n",
    "        probabilities = exp_q / np.sum(exp_q)\n",
    "        return available_actions[np.random.choice(len(available_actions), p=probabilities)]\n",
    "    \n",
    "    def load_or_initialize_q_table(self):\n",
    "        \"\"\" Check if Q-table file exists, load it, otherwise initialize a new Q-table. \"\"\"\n",
    "        if os.path.exists(self.q_table_file):\n",
    "            return np.load(self.q_table_file)\n",
    "        else:\n",
    "            return np.random.rand(self.world.width, self.world.height, len(ACTIONS))\n",
    "    \n",
    "    def update_position(self):\n",
    "        \"\"\" Move based on softmax action selection \"\"\"\n",
    "        if not self.active or self.world.is_terminal(*self.position):\n",
    "            return\n",
    "        \n",
    "        search_directions = {\n",
    "            \"up\": (self.position[0], self.position[1] - 1),\n",
    "            \"down\": (self.position[0], self.position[1] + 1),\n",
    "            \"left\": (self.position[0] - 1, self.position[1]),\n",
    "            \"right\": (self.position[0] + 1, self.position[1])\n",
    "        }\n",
    "        \n",
    "        available_actions = [pos for action, pos in search_directions.items()\n",
    "                             if not self.world.is_obstacle(*pos) and 0 <= pos[0] < self.world.width and 0 <= pos[1] < self.world.height]\n",
    "        \n",
    "        if available_actions:\n",
    "            best_move = self.softmax_policy(available_actions)\n",
    "            self.position = best_move\n",
    "            self.cumulative_reward += self.compute_reward(best_move)\n",
    "        \n",
    "        self.visited_positions.append(self.position)\n",
    "        \n",
    "        if self.world.is_terminal(*self.position):\n",
    "            self.active = False\n"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image-3.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy og action selection\n",
    "\n",
    "![image.png](attachment:image.png) ![image-2.png](attachment:image-2.png) ![image-3.png](attachment:image-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to keep track of current estimates\n",
    "Q = np.random.rand(world.width, world.height, len(ACTIONS))\n",
    "\n",
    "# Define a function for the Boltzmann policy\n",
    "def swarmdrone_policy(x, y, Q, beta=1.0):\n",
    "    # Get Q-values for all actions at state (x, y)\n",
    "    q_values = Q[x, y, :]\n",
    "\n",
    "    # Compute the exponentiated values\n",
    "    exp_q = np.exp(beta * q_values)\n",
    "\n",
    "    # Normalize to get probabilities\n",
    "    probabilities = exp_q / np.sum(exp_q)\n",
    "\n",
    "    # Return a dictionary mapping actions to their probabilities\n",
    "    return {action: prob for action, prob in zip(ACTIONS, probabilities)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxncUHvz1_Hb"
   },
   "source": [
    "## Basic examples: World, obstacles, rewards and terminals\n",
    "\n",
    "Below are some examples to illustrate how the ```World``` class works.\n",
    "\n",
    "First, we create a 4x4 world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dL-XC6-aN6ss",
    "outputId": "1ca1d560-5d87-4820-eba5-93e3b94c20d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "world = World(4, 4)\n",
    "\n",
    "# Note, that we have to transpose the 2D array (.T) for (x,y)\n",
    "# to match the convention when displayed\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1C8CRt92wqf"
   },
   "source": [
    "Obstacles and terminals are all represented as single characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKlTP05N2xEe",
    "outputId": "5170d9c4-a30f-46c6-cadf-6da30def53e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obstacles: #\n",
      "Terminals: ('+', '-')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Obstacles: {OBSTACLES}\")\n",
    "print(f\"Terminals: {TERMINALS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c30K2bDd3yyI"
   },
   "source": [
    "Rewards are also represented as characters in the world, but they have an associated value (note that defining a value for an empty space \"  \" is equivalent to the agent receiving that reward each time a move is made):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54TwLFOb3y-1",
    "outputId": "8501ede1-5773-409d-edd4-ba1031edad07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: {' ': 0, '.': 0.1, '+': 10, '-': -10}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rewards: {REWARDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcG0mAYl4ux7"
   },
   "source": [
    "To assign rewards to terminal states, just use the same character in the ```REWARDS``` dict and in the ```TERMINALS``` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "7MinV0Bg49dB",
    "outputId": "8e48ee49-439b-4a5d-b561-8d9a27f0a873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal + has reward 10\n",
      "Terminal - has reward -10\n",
      "[['+' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' '-']]\n"
     ]
    }
   ],
   "source": [
    "for t in TERMINALS:\n",
    "  print(f\"Terminal {t} has reward {REWARDS[t]}\")\n",
    "\n",
    "world.add_terminal(0, 0, \"+\")\n",
    "world.add_terminal(3, 3, \"-\")\n",
    "\n",
    "\n",
    "print(world.grid.T)\n",
    "\n",
    "# An alternative way of displaying the world using pandas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94bkx57KdvGD"
   },
   "source": [
    "## Policies ($\\pi$)\n",
    "\n",
    "Recall that a policy, $\\pi(a|s) = \\Pr(A_t = a | S_t = s)$, maps states to action probabilities. In the code below, we let policies return the probabilities of each possible action given a state. States are $(x, y)$ coordinates and the policy must return action probabilities as a dict where the action is the ```key``` and the corresponding ```value``` is the probability of taking that action in the given state. Deterministic policies, for instance, return a dict with only one entry (e.g. ```{ \"up\": 1 } ``` if the action for the current state is ```up```).\n",
    "\n",
    "A random policy can be defined as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vhf3hfg9eCYS",
    "outputId": "af326e4c-db54-4b82-977f-6d2ef451383e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n"
     ]
    }
   ],
   "source": [
    "def equiprobable_random_policy(x, y):\n",
    "  return { k : 1 / len(ACTIONS) for k in ACTIONS }\n",
    "\n",
    "# Example (since the action probabilities do not depend on the state in this\n",
    "# basic policy, we can just call it for state (0, 0)):\n",
    "print(equiprobable_random_policy(0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGK61Lk8dcd2"
   },
   "source": [
    "## Iterative policy evaluation\n",
    "\n",
    "\n",
    "Iterative policy evaluation takes a ```World```, a discount factor, $\\gamma$ (```gamma```, defined above in the ```World``` code cell), a policy, $\\pi$, and a threshold, $\\theta$ (```theta```), that determines when to stop the iteration. You can also specify a maximum number of iterations which can be useful for debugging using the ```max_iterations``` argument.\n",
    "\n",
    "**IMPORTANT:** Remember that in iterative policy evaluation, we just learn state values ($V_\\pi$) given a policy $\\pi$. We are **not** trying to learn a policy.\n",
    "\n",
    "(see page 74-75 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for an explanation and the algorithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9fpxspw3b8U1"
   },
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(world, policy, theta=1e-5, max_iterations=1e3):\n",
    "\n",
    "  # Our initial estimates for all states in the world is 0:\n",
    "  V = np.full((world.width, world.height), 0.0)\n",
    "\n",
    "  while True:\n",
    "    # delta keeps track of the largest change in one iteration, so we set it to\n",
    "    # 0 at the start of each iteration:\n",
    "    delta = 0\n",
    "\n",
    "    # Loop over all states (x,y)\n",
    "    for y in range(world.height):\n",
    "      for x in range(world.width):\n",
    "        if not world.is_obstacle(x, y):\n",
    "          # Get action probabilities for the current state:\n",
    "          actions = policy(x, y)\n",
    "\n",
    "          # v is the new estimate that will be updated in the loop:\n",
    "          v = 0\n",
    "\n",
    "          # loop over all actions that our policy says that we can perform\n",
    "          # in the current state:\n",
    "          for action, action_prob in actions.items():\n",
    "            # For each action, get state transition probabilities and\n",
    "            # accumulate in v rewards weighted with action and state transition\n",
    "            # probabilities:\n",
    "            for (xi, yi), state_prob in world.get_state_transition_probabilities((x, y), action).items():\n",
    "              v += action_prob * state_prob * (world.get_reward(xi, yi) + gamma * V[xi, yi])\n",
    "\n",
    "          # update delta (largest change in estimate so far)\n",
    "          delta = max(delta, abs(v - V[x, y]))\n",
    "          V[x, y] = v\n",
    "\n",
    "    # check if current state value estimates are close enought to end:\n",
    "    if delta <= theta:\n",
    "      break\n",
    "\n",
    "    max_iterations -= 1\n",
    "    if max_iterations == 0:\n",
    "      break\n",
    "\n",
    "  # Return the state value estimates\n",
    "  return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlZK9xU65iS2"
   },
   "source": [
    "## Implementation of Example 4.1 from the book\n",
    "\n",
    "Below, you can see the implementation of Example 4.1 on page 76 in the book [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQG26oW05idR",
    "outputId": "5d3998a6-4022-4bc6-e6c8-a87045efc1f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['+' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "# World is 4x4\n",
    "world = World(4, 4)\n",
    "\n",
    "# Rewards are -1 for each move (including when hitting a terminal state, \"+\"):\n",
    "REWARDS = {\" \": -1, \"+\": -1}\n",
    "\n",
    "\n",
    "# Add terminal states in two corners\n",
    "world.add_terminal(0, 0, \"+\")\n",
    "world.add_terminal(3, 3, \"+\")\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "tMudqC-OBUWo",
    "outputId": "24738d14-0186-445b-e1fa-12053ee05c48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-14.000</td>\n",
       "      <td>-20.000</td>\n",
       "      <td>-22.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-14.000</td>\n",
       "      <td>-18.000</td>\n",
       "      <td>-20.000</td>\n",
       "      <td>-20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-20.000</td>\n",
       "      <td>-20.000</td>\n",
       "      <td>-18.000</td>\n",
       "      <td>-14.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-22.000</td>\n",
       "      <td>-20.000</td>\n",
       "      <td>-14.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3\n",
       "0   0.000 -14.000 -20.000 -22.000\n",
       "1 -14.000 -18.000 -20.000 -20.000\n",
       "2 -20.000 -20.000 -18.000 -14.000\n",
       "3 -22.000 -20.000 -14.000   0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V = iterative_policy_evaluation(world, equiprobable_random_policy)\n",
    "\n",
    "display(pd.DataFrame(V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNagzYkLaqAe"
   },
   "source": [
    "## Exercise - policy and discount factor\n",
    "\n",
    "Experiment with example 4.1: what effect does it have to change the policy, e.g. so that an agent always goes left or always goes right? What effect does it have on state values to change the value of the discount factor (```gamma```)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "krYhqYtTOGhD"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>-1.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0  0.000 -1.000 -1.500 -1.750\n",
       "1 -2.000 -2.000 -2.000 -2.000\n",
       "2 -2.000 -2.000 -2.000 -2.000\n",
       "3 -2.000 -2.000 -2.000  0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.5\n",
    "def left_policy(x, y):\n",
    "  \n",
    "  return {'left' : 1}\n",
    "\n",
    "\n",
    "V = iterative_policy_evaluation(world, left_policy)\n",
    "\n",
    "display(pd.DataFrame(V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CtRf7JInBq4"
   },
   "source": [
    "Try to write a policy that is deterministic, but where the action performed differs between states. You can implement it as a two-dimensional array with the dimensions corresponding to the world dimensions and have each entry be an action for the corresponding state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1,999.000</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2,000.000</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3\n",
       "0      0.000 -1,000.000 -1,000.000 -1,000.000\n",
       "1     -1.000     -2.000     -3.000     -2.000\n",
       "2 -1,999.000     -5.000     -4.000     -1.000\n",
       "3 -2,000.000     -6.000     -1.000      0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  +         \n",
       "1            \n",
       "2            \n",
       "3           +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 1\n",
    "\n",
    "def deterministic_policy(x,y):\n",
    "    policy_grid = [\n",
    "    [\"up\", \"up\", \"right\", \"right\"],\n",
    "    [\"up\", \"left\", \"right\", \"down\"],\n",
    "    [\"down\", \"right\", \"up\", \"down\"],\n",
    "    [\"up\", \"up\", \"right\", \"right\"]\n",
    "    ]\n",
    "\n",
    "    action = policy_grid[y][x]\n",
    "    return {action : 1}\n",
    "\n",
    "V = iterative_policy_evaluation(world, deterministic_policy)\n",
    "display(pd.DataFrame(V.T))\n",
    "display(pd.DataFrame(world.grid.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHDCfB_uiuxS"
   },
   "source": [
    "## Exercise - stochasticity\n",
    "\n",
    "You can adjust the degree of stochasticity in the environment by setting the global variable ```rand_move_probability``` (the probability of the world ignoring an action and performing a random move instead). What effect does stochasticity have on the state-value estimates?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "pel0dbaWnh1D"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-3.000</td>\n",
       "      <td>-2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1,999.000</td>\n",
       "      <td>-5.000</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2,000.000</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3\n",
       "0      0.000 -1,000.000 -1,000.000 -1,000.000\n",
       "1     -1.000     -2.000     -3.000     -2.000\n",
       "2 -1,999.000     -5.000     -4.000     -1.000\n",
       "3 -2,000.000     -6.000     -1.000      0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 1\n",
    "\n",
    "def deterministic_policy(x,y):\n",
    "    policy_grid = [\n",
    "    [\"up\", \"up\", \"right\", \"right\"],\n",
    "    [\"up\", \"left\", \"right\", \"down\"],\n",
    "    [\"down\", \"right\", \"up\", \"down\"],\n",
    "    [\"up\", \"up\", \"right\", \"right\"]\n",
    "    ]\n",
    "\n",
    "    action = policy_grid[y][x]\n",
    "    return {action : 1}\n",
    "rand_move_probability = 0\n",
    "V = iterative_policy_evaluation(world, deterministic_policy)\n",
    "display(pd.DataFrame(V.T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHC0cosVjU4t"
   },
   "source": [
    "## Exercise - robot, cake and mouse trap\n",
    "\n",
    "Implement a robot, cake and mouse trap example and compute state value estimates under different policies (equiprobable, always right, always right:50% or up:50%) with and without stochasticity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ujejW_93jzQK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1,999.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-13.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2,000.000</td>\n",
       "      <td>-11.000</td>\n",
       "      <td>-12.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3\n",
       "0      0.000 -1,000.000 -1,000.000 -1,000.000\n",
       "1     10.000      0.000      0.000      9.000\n",
       "2 -1,999.000    -10.000    -13.000     10.000\n",
       "3 -2,000.000    -11.000    -12.000      0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 1\n",
    "\n",
    "def deterministic_policy(x,y):\n",
    "    \n",
    "    \n",
    "    policy_grid = [\n",
    "    [\"up\", \"up\", \"right\", \"right\"],\n",
    "    [\"up\", \"left\", \"right\", \"down\"],\n",
    "    [\"down\", \"up\", \"down\", \"down\"],\n",
    "    [\"up\", \"up\", \"left\", \"right\"]\n",
    "    ]\n",
    "\n",
    "    action = policy_grid[y][x]\n",
    "    return {action : 1}\n",
    "\n",
    "world.add_terminal(2,1,'+')\n",
    "world.add_terminal(1,1,'-')\n",
    "\n",
    "REWARDS = {' ' : -1 , '+' : +10 , '-' : -10}\n",
    "\n",
    "V = iterative_policy_evaluation(world, deterministic_policy)\n",
    "display(pd.DataFrame(V.T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_kjVW7UkDhO"
   },
   "source": [
    "## Exercise - action-value function\n",
    "\n",
    "Based on a set of calculated state values, try to implement an action value function, that is $q_\\pi(s, a)$ (if in doubt, see page 78 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)). Note: you have to use the ```get_state_transition_probabilities()``` method on ```World``` to be able to handle stochastic environments where performing ```a``` does not lead to a deterministic outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "qiCGsZ7klPzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 4.375\n",
      "('up', 'down', 'left', 'right')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "      <td>-1,000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1,999.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-13.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2,000.000</td>\n",
       "      <td>-11.000</td>\n",
       "      <td>-12.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3\n",
       "0      0.000 -1,000.000 -1,000.000 -1,000.000\n",
       "1     10.000      0.000      0.000      9.000\n",
       "2 -1,999.000    -10.000    -13.000     10.000\n",
       "3 -2,000.000    -11.000    -12.000      0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_state = (2,2)\n",
    "gamma = 1\n",
    "\n",
    "\n",
    "rand_move_probability = 0.5\n",
    "# def action_value(world, V, state, action):\n",
    "#   p = world.get_state_transition_probabilities(state, action)\n",
    "#   first_key, state_prop = next(iter(p.items()))\n",
    "#   reward = world.get_reward(first_key[0], first_key[1])\n",
    "#   q = state_prop * reward + gamma * V[first_key[0],first_key[1]]\n",
    "\n",
    "#   print(\"Hvor gr vi til?\", first_key[0], first_key[1])\n",
    "#   print(\"state prop\", state_prop)\n",
    "#   print(\"her er reward\", reward)\n",
    "#   print(\"her er p\",p)\n",
    "#   print(\"her er Q\", q)\n",
    "#   pass\n",
    "\n",
    "V_left = iterative_policy_evaluation(world, left_policy)\n",
    "\n",
    "def action_value(world, V, state, action):\n",
    "  q = 0\n",
    "  for (xi, yi), state_prop in world.get_state_transition_probabilities((state), action).items():\n",
    "    q += state_prop * (world.get_reward(xi, yi) + gamma * V[xi, yi])\n",
    "  return q\n",
    "\n",
    "test = action_value(world, V, test_state, 'up')\n",
    "print(\"test\" ,test)\n",
    "print(ACTIONS)\n",
    "display(pd.DataFrame(V.T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hveeaq7zn2g5"
   },
   "source": [
    "## Exercise - policy iteration\n",
    "\n",
    "You are now ready to implement policy iteration. That is, first estimate state values under a given policy, then improve the policy based on those estimates and action values, estimate state values again, and so on. See page 80 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "\n",
    "You will need an explicit representation of your policy that you can easily change.\n",
    "\n",
    "Test your implementation and print out the policies found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "h9WW80CWoFPh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>6.137</td>\n",
       "      <td>7.732</td>\n",
       "      <td>4.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.541</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.372</td>\n",
       "      <td>2.331</td>\n",
       "      <td>7.714</td>\n",
       "      <td>8.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.309</td>\n",
       "      <td>3.619</td>\n",
       "      <td>7.787</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3\n",
       "0 0.000 6.137 7.732 4.697\n",
       "1 5.541 0.000 0.000 7.969\n",
       "2 2.372 2.331 7.714 8.270\n",
       "3 1.309 3.619 7.787 0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>left</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>up</td>\n",
       "      <td>right</td>\n",
       "      <td>up</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3\n",
       "0  right   left   down   down\n",
       "1     up  right  right   left\n",
       "2     up  right     up   down\n",
       "3  right  right  right  right"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  +         \n",
       "1     -  +   \n",
       "2            \n",
       "3           +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Implement your code here\n",
    "\n",
    "rand_move_probability = 0.5\n",
    "gamma = 0.8\n",
    "\n",
    "MatPol = np.full((4,4), 'right')\n",
    "\n",
    "def explicit_policy(x,y):\n",
    "    return {MatPol [x,y] : 1}\n",
    "\n",
    "\n",
    "def policy_iteration(world, policy):\n",
    "    \n",
    "    while(True):\n",
    "        policy_stable = True\n",
    "        V = iterative_policy_evaluation(world, explicit_policy)\n",
    "        \n",
    "        for x in range(world.width):\n",
    "            for y in range(world.height):\n",
    "                old_action = policy[x,y]\n",
    "\n",
    "                for action in ACTIONS:\n",
    "                    \n",
    "                    if action_value(world, V, (x,y), action) > action_value(world, V, (x,y), old_action):\n",
    "                        policy[x,y] = action\n",
    "                if (old_action != policy[x,y]):\n",
    "                    policy_stable = False\n",
    "        if policy_stable:\n",
    "            display(pd.DataFrame(V.T))\n",
    "            display(pd.DataFrame(policy.T))\n",
    "            display(pd.DataFrame(world.grid.T))\n",
    "            return\n",
    "            \n",
    "policy_iteration(world, MatPol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijYaKkBloGdN"
   },
   "source": [
    "## Exercise - value iteration\n",
    "\n",
    "Value iteration is much more effecient than policy iteration. Implement value iteration below. See page 83 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n",
    "\n",
    "Test your implementation and display the policies found (i.e., a grid with the perferred action in each cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "0lw1ZXVroJUa"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

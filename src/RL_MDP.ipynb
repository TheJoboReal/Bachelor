{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.3)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.17.0 fsspec-2025.2.0 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas torch\n",
    "! python -m pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys          # we use sys to get the max value of a float\n",
    "import pandas as pd # we only use pandas for displaying tables nicely\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": -1, \".\": 2, \"+\": 30, \"-\": -10}\n",
    "OBSTACLES = (\"#\")\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "class World:\n",
    "  def __init__(self, width, height):\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    # Create an empty world where the agent can move to all cells\n",
    "    self.grid = np.full((width, height), '.', dtype='U1')\n",
    "\n",
    "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "    \"\"\"\n",
    "    Create an obstacle in either a single cell or rectangle.\n",
    "    \"\"\"\n",
    "    if end_x == None: end_x = start_x\n",
    "    if end_y == None: end_y = start_y\n",
    "\n",
    "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "  def add_reward(self, x, y, reward):\n",
    "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "    self.grid[x, y] = reward\n",
    "\n",
    "  def is_obstacle(self, x, y):\n",
    "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "      return True\n",
    "    else:\n",
    "      return self.grid[x ,y] in OBSTACLES\n",
    "\n",
    "\n",
    "  def get_reward(self, x, y):\n",
    "    \"\"\"\n",
    "    Return the reward associated with a given location\n",
    "    \"\"\"\n",
    "    return REWARDS[self.grid[x, y]]\n",
    "\n",
    "  def get_next_state(self, current_state, action):\n",
    "    \"\"\"\n",
    "    Get the next state given a current state and an action. The outcome can be\n",
    "    stochastic  where rand_move_probability determines the probability of\n",
    "    ignoring the action and performing a random move.\n",
    "    \"\"\"\n",
    "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "    x, y = current_state\n",
    "\n",
    "    # Check of a random action should be performed:\n",
    "    if np.random.rand() < rand_move_probability:\n",
    "      action = np.random.choice(ACTIONS)\n",
    "\n",
    "    if action == \"up\":      y -= 1\n",
    "    elif action == \"down\":  y += 1\n",
    "    elif action == \"left\":  x -= 1\n",
    "    elif action == \"right\": x += 1\n",
    "\n",
    "    # If the next state is an obstacle, stay in the current state\n",
    "    return (x, y) if not self.is_obstacle(x, y) else current_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real Time Markov Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9\n",
       "0  .  .  .  .  .  .  .  .  .  .\n",
       "1  .  .  .  .  .  .  .  .  .  .\n",
       "2  .  +  .  +  .  .  .  .  .  .\n",
       "3  .  .  +  .  +  .  .  .  .  +\n",
       "4  .  .  .  .  .  .  .  .  .  .\n",
       "5  .  .  .  .  .  .  .  .  .  .\n",
       "6  .  .  .  .  .  .  .  .  .  .\n",
       "7  .  .  .  .  .  .  .  .  .  .\n",
       "8  .  .  .  .  .  .  .  +  .  .\n",
       "9  .  .  .  .  .  .  .  .  .  +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = World(10, 10)\n",
    "\n",
    "world.add_reward(2,3, \"+\")\n",
    "world.add_reward(1,2, \"+\")\n",
    "world.add_reward(3,2, \"+\")\n",
    "world.add_reward(4,3, \"+\")\n",
    "world.add_reward(9,3, \"+\")\n",
    "world.add_reward(9,9, \"+\")\n",
    "world.add_reward(7,8, \"+\")\n",
    "\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: left, Next State: (0, 0), Reward: 2, Total Reward: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGzCAYAAAASUAGgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG1RJREFUeJzt3XtwVPX5x/HPJubWmKQRSBTZyEU7UcAbCamkVZFIxkEqWBUtKmCHWiYJl9SOoQ5QBiFixUlHLgrjIKOJUYsIOmInExVFoAmiFLWKFMUIchPMRsBId7+/P/yxNSVoNuRhd8P7NXP+8OScPU+2uu+ec3Y3HuecEwAAHSwm3AMAADonAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDdHKffvqpPB6PHnrooXCPgtMMgUFUWbhwoTwej/Ly8sI9SqsWLlyoJ554os3bezye4BITE6Pu3btr6NChev3110M+9ssvv6w///nPIe8HWCEwiCqVlZXq2bOn6urqtG3btnCPc5xQAyNJ1157rZ588kktW7ZMv//97/XPf/5T11xzjVavXh3S47z88suaOXNmSPsAlggMosYnn3yidevW6eGHH1a3bt1UWVkZ7pE6xM9+9jPdfvvtuuOOOzR9+nTV1NTIOaeKiopwjwacFAKDqFFZWan09HQNGzZMN9100wkD8+WXX+qOO+5QamqqfvrTn2rMmDHavHmzPB7PcWcXH374oW666SadddZZSkxMVE5OjlatWtVimyeeeEIej0dvvfWWSktL1a1bNyUnJ2vkyJHat29fcLuePXvq/fff15o1a4KXva6++uqQf8/+/fura9eu+uSTTyRJb775pm6++WZlZWUpISFBXq9XU6ZM0ZEjR4L7jB07VgsWLJDU8rLb/1q8eLH69OmjhIQE5ebmqr6+PuT5gLY6I9wDAG1VWVmpG2+8UfHx8brtttu0aNEi1dfXKzc3N7hNIBDQ8OHDVVdXpwkTJig7O1srV67UmDFjjnu8999/X/n5+Tr33HNVVlam5ORkPfvssxoxYoSWL1+ukSNHtti+pKRE6enpmjFjhj799FNVVFSouLhYzzzzjCSpoqJCJSUlOvPMM3XfffdJkjIzM0P+PQ8ePKiDBw/q/PPPlyQ999xzOnz4sCZMmKAuXbqorq5OjzzyiD7//HM999xzkqS7775bu3btUk1NjZ588slWH7eqqkpNTU26++675fF49OCDD+rGG2/U9u3bFRcXF/KcwI9yQBTYuHGjk+Rqamqcc84FAgHXo0cPN2nSpBbbLV++3ElyFRUVwXV+v99dc801TpJbunRpcP2QIUNc//793TfffBNcFwgE3KBBg9wFF1wQXLd06VInyRUUFLhAIBBcP2XKFBcbG+u++uqr4Lq+ffu6q666qs2/lyT329/+1u3bt8/t3bvX/eMf/3BDhgxxkty8efOcc84dPnz4uP3Ky8udx+NxO3bsCK4rKipyrf0n/cknnzhJrkuXLu7AgQPB9StXrnSS3IsvvtjmeYFQcIkMUaGyslKZmZkaPHiwpO8uA40aNUrV1dXy+/3B7V555RXFxcVp/PjxwXUxMTEqKipq8XgHDhzQq6++qltuuUVNTU3av3+/9u/fry+//FKFhYX6+OOPtXPnzhb7/O53v2tx2emXv/yl/H6/duzYcVK/2+OPP65u3bopIyNDeXl5wUtxkydPliQlJSUFtz106JD279+vQYMGyTmnd955p83HGTVqlNLT01vML0nbt28/qfmBE+ESGSKe3+9XdXW1Bg8eHLwvIUl5eXmaN2+eamtrNXToUEnSjh07dM455+gnP/lJi8c4drnpmG3btsk5p2nTpmnatGmtHnfv3r0699xzg/+clZXV4ufHXqwPHjzY/l9O0g033KDi4mJ5PB6lpKSob9++Sk5ODv78s88+0/Tp07Vq1arjjtXY2Njm41jND5wIgUHEe/XVV/XFF1+ourpa1dXVx/28srIyGJi2CgQCkqR77rlHhYWFrW7zv1GKjY1tdTt3kn91vEePHiooKGj1Z36/X9dee60OHDige++9V9nZ2UpOTtbOnTs1duzY4O/RFlbzAydCYBDxKisrlZGREXyX1Pc9//zzWrFihR599FElJSXpvPPO02uvvabDhw+3OIv538/M9O7dW5IUFxd3whf39mjtnVsnY8uWLdq6dauWLVumO++8M7i+pqbG/NjAyeIeDCLakSNH9Pzzz+v666/XTTfddNxSXFyspqam4FuLCwsLdfToUS1ZsiT4GIFA4Lg4ZWRk6Oqrr9Zjjz2mL7744rjjfv/tx6FITk7WV1991a59W3PsrOP7ZxnOOf31r39t9diSOvT4wMngDAYRbdWqVWpqatKvfvWrVn/+85//PPihy1GjRmnEiBEaOHCg/vCHP2jbtm3Kzs7WqlWrdODAAUkt/1/+ggUL9Itf/EL9+/fX+PHj1bt3b+3Zs0fr16/X559/rs2bN4c874ABA7Ro0SLdf//9Ov/885WRkaFrrrmmfb+8pOzsbPXp00f33HOPdu7cqdTUVC1fvrzV+yYDBgyQJE2cOFGFhYWKjY3Vrbfe2u5jAyctnG9hA37M8OHDXWJiojt06NAJtxk7dqyLi4tz+/fvd845t2/fPveb3/zGpaSkuLS0NDd27Fj31ltvOUmuurq6xb7//ve/3Z133unOPvtsFxcX584991x3/fXXu7/97W/BbY69Tbm+vr7Fvq+99pqT5F577bXgut27d7thw4a5lJQUJ+lH37IsyRUVFf3gNh988IErKChwZ555puvatasbP36827x583Fvu/7Pf/7jSkpKXLdu3ZzH4wm+ZfnY25T/8pe/tHr8GTNm/ODxgfbyOMcdPnR+L7zwgkaOHKm1a9cqPz8/3OMApwUCg07nyJEjLT474vf7NXToUG3cuFG7d+9u8TMAdrgHg06npKRER44c0RVXXKHm5mY9//zzWrdunebMmUNcgFOIMxh0OlVVVZo3b562bdumb775Rueff74mTJig4uLicI8GnFYIDADABJ+DAQCYIDAAABOn/CZ/IBDQrl27lJKSwldbAECUcc6pqalJ3bt3V0zMD5+jnPLA7Nq1S16v91QfFgDQgRoaGtSjR48f3OaUByYlJeVUH7JNQvnacwA4Xfl8Pnm93ja9lp/ywETqZbHU1NRwjwAAUaMtr+Xc5AcAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCiXYFZsGCBevbsqcTEROXl5amurq6j5wIARLmQA/PMM8+otLRUM2bM0KZNm3TJJZeosLBQe/futZgPABClPM45F8oOeXl5ys3N1fz58yV99wfEvF6vSkpKVFZW9qP7+3w+paWltW9aQyE+DQBwWjr2Gt7Y2Pij30If0hnMt99+q7ffflsFBQX/fYCYGBUUFGj9+vWt7tPc3Cyfz9diAQB0fiEFZv/+/fL7/crMzGyxPjMzU7t37251n/LycqWlpQUX/polAJwezN9FNnXqVDU2NgaXhoYG60MCACJASH/RsmvXroqNjdWePXtarN+zZ4/OPvvsVvdJSEhQQkJC+ycEAESlkM5g4uPjNWDAANXW1gbXBQIB1dbW6oorrujw4QAA0SukMxhJKi0t1ZgxY5STk6OBAweqoqJChw4d0rhx4yzmAwBEqZADM2rUKO3bt0/Tp0/X7t27demll+qVV1457sY/AOD0FvLnYE4Wn4MBgOhl9jkYAADaisAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgImQv+yyo7Tle2wAANGLMxgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACZCCkx5eblyc3OVkpKijIwMjRgxQh999JHVbACAKBZSYNasWaOioiJt2LBBNTU1Onr0qIYOHapDhw5ZzQcAiFIe55xr78779u1TRkaG1qxZoyuvvLJN+/h8PqWlpamxsVGpqantPTQAIAxCeQ0/42QO1NjYKEk666yzTrhNc3OzmpubWwwHAOj82n2TPxAIaPLkycrPz1e/fv1OuF15ebnS0tKCi9frbe8hAQBRpN2XyCZMmKDVq1dr7dq16tGjxwm3a+0Mxuv1cokMAKKQ+SWy4uJivfTSS3rjjTd+MC6SlJCQoISEhPYcBgAQxUIKjHNOJSUlWrFihV5//XX16tXLai4AQJQLKTBFRUWqqqrSypUrlZKSot27d0uS0tLSlJSUZDIgACA6hXQPxuPxtLp+6dKlGjt2bJseg7cpA0D0MrsHcxIfmQEAnGb4LjIAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBg4qQC88ADD8jj8Wjy5MkdNA4AoLNod2Dq6+v12GOP6eKLL+7IeQAAnUS7AvP1119r9OjRWrJkidLT0zt6JgBAJ9CuwBQVFWnYsGEqKCj40W2bm5vl8/laLACAzu+MUHeorq7Wpk2bVF9f36bty8vLNXPmzJAHAwBEt5DOYBoaGjRp0iRVVlYqMTGxTftMnTpVjY2NwaWhoaFdgwIAoovHOefauvELL7ygkSNHKjY2NrjO7/fL4/EoJiZGzc3NLX7WGp/Pp7S0NDU2Nio1NbX9kwMATrlQXsNDukQ2ZMgQbdmypcW6cePGKTs7W/fee++PxgUAcPoIKTApKSnq169fi3XJycnq0qXLcesBAKc3PskPADAR8rvI/tfrr7/eAWMAADobzmAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACAiZADs3PnTt1+++3q0qWLkpKS1L9/f23cuNFiNgBAFDsjlI0PHjyo/Px8DR48WKtXr1a3bt308ccfKz093Wo+AECUCikwc+fOldfr1dKlS4PrevXq1eFDAQCiX0iXyFatWqWcnBzdfPPNysjI0GWXXaYlS5b84D7Nzc3y+XwtFgBA5xdSYLZv365Fixbpggsu0N///ndNmDBBEydO1LJly064T3l5udLS0oKL1+s96aEBAJHP45xzbd04Pj5eOTk5WrduXXDdxIkTVV9fr/Xr17e6T3Nzs5qbm4P/7PP55PV61djYqNTU1JMYHQBwqvl8PqWlpbXpNTykM5hzzjlHF110UYt1F154oT777LMT7pOQkKDU1NQWCwCg8wspMPn5+froo49arNu6davOO++8Dh0KABD9QgrMlClTtGHDBs2ZM0fbtm1TVVWVFi9erKKiIqv5AABRKqTA5ObmasWKFXr66afVr18/zZo1SxUVFRo9erTVfACAKBXSTf6OEMoNIgBAZDG7yQ8AQFsRGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEyEFxu/3a9q0aerVq5eSkpLUp08fzZo1S845q/kAAFHqjFA2njt3rhYtWqRly5apb9++2rhxo8aNG6e0tDRNnDjRakYAQBQKKTDr1q3TDTfcoGHDhkmSevbsqaefflp1dXUmwwEAoldIl8gGDRqk2tpabd26VZK0efNmrV27Vtddd90J92lubpbP52uxAAA6v5DOYMrKyuTz+ZSdna3Y2Fj5/X7Nnj1bo0ePPuE+5eXlmjlz5kkPCgCILiGdwTz77LOqrKxUVVWVNm3apGXLlumhhx7SsmXLTrjP1KlT1djYGFwaGhpOemgAQOTzuBDeAub1elVWVqaioqLguvvvv19PPfWUPvzwwzY9hs/nU1pamhobG5Wamhr6xACAsAnlNTykM5jDhw8rJqblLrGxsQoEAqFPCQDo1EK6BzN8+HDNnj1bWVlZ6tu3r9555x09/PDDuuuuu6zmAwBEqZAukTU1NWnatGlasWKF9u7dq+7du+u2227T9OnTFR8f36bH4BIZAESvUF7DQwpMRyAwABC9zO7BAADQVgQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAIAJAgMAMEFgAAAmCAwAwASBAQCYIDAAABMEBgBggsAAAEwQGACACQIDADBBYAAAJggMAMAEgQEAmCAwAAATBAYAYILAAABMEBgAgAkCAwAwQWAAACYIDADAxBmn+oDOOUmSz+c71YcGAJykY6/dx17Lf8gpD0xTU5Mkyev1nupDAwA6SFNTk9LS0n5wG49rS4Y6UCAQ0K5du5SSkiKPx9Pux/H5fPJ6vWpoaFBqamoHTti58Dy1Dc9T2/A8tU1nfp6cc2pqalL37t0VE/PDd1lO+RlMTEyMevTo0WGPl5qa2un+B7TA89Q2PE9tw/PUNp31efqxM5djuMkPADBBYAAAJqI2MAkJCZoxY4YSEhLCPUpE43lqG56ntuF5ahuep++c8pv8AIDTQ9SewQAAIhuBAQCYIDAAABMEBgBggsAAAExEbWAWLFignj17KjExUXl5eaqrqwv3SBGlvLxcubm5SklJUUZGhkaMGKGPPvoo3GNFtAceeEAej0eTJ08O9ygRZ+fOnbr99tvVpUsXJSUlqX///tq4cWO4x4oofr9f06ZNU69evZSUlKQ+ffpo1qxZbfpSyM4qKgPzzDPPqLS0VDNmzNCmTZt0ySWXqLCwUHv37g33aBFjzZo1Kioq0oYNG1RTU6OjR49q6NChOnToULhHi0j19fV67LHHdPHFF4d7lIhz8OBB5efnKy4uTqtXr9YHH3ygefPmKT09PdyjRZS5c+dq0aJFmj9/vv71r39p7ty5evDBB/XII4+Ee7SwicrPweTl5Sk3N1fz58+X9N0XaHq9XpWUlKisrCzM00Wmffv2KSMjQ2vWrNGVV14Z7nEiytdff63LL79cCxcu1P33369LL71UFRUV4R4rYpSVlemtt97Sm2++Ge5RItr111+vzMxMPf7448F1v/71r5WUlKSnnnoqjJOFT9SdwXz77bd6++23VVBQEFwXExOjgoICrV+/PoyTRbbGxkZJ0llnnRXmSSJPUVGRhg0b1uLfKfzXqlWrlJOTo5tvvlkZGRm67LLLtGTJknCPFXEGDRqk2tpabd26VZK0efNmrV27Vtddd12YJwufU/5tyidr//798vv9yszMbLE+MzNTH374YZimimyBQECTJ09Wfn6++vXrF+5xIkp1dbU2bdqk+vr6cI8SsbZv365FixaptLRUf/rTn1RfX6+JEycqPj5eY8aMCfd4EaOsrEw+n0/Z2dmKjY2V3+/X7NmzNXr06HCPFjZRFxiErqioSO+9957Wrl0b7lEiSkNDgyZNmqSamholJiaGe5yIFQgElJOTozlz5kiSLrvsMr333nt69NFHCcz3PPvss6qsrFRVVZX69u2rd999V5MnT1b37t1P2+cp6gLTtWtXxcbGas+ePS3W79mzR2effXaYpopcxcXFeumll/TGG2906N/h6Qzefvtt7d27V5dffnlwnd/v1xtvvKH58+erublZsbGxYZwwMpxzzjm66KKLWqy78MILtXz58jBNFJn++Mc/qqysTLfeeqskqX///tqxY4fKy8tP28BE3T2Y+Ph4DRgwQLW1tcF1gUBAtbW1uuKKK8I4WWRxzqm4uFgrVqzQq6++ql69eoV7pIgzZMgQbdmyRe+++25wycnJ0ejRo/Xuu+8Sl/+Xn59/3Fvct27dqvPOOy9ME0Wmw4cPH/cXHmNjYxUIBMI0UfhF3RmMJJWWlmrMmDHKycnRwIEDVVFRoUOHDmncuHHhHi1iFBUVqaqqSitXrlRKSop2794t6bu/RJeUlBTm6SJDSkrKcfekkpOT1aVLF+5Vfc+UKVM0aNAgzZkzR7fccovq6uq0ePFiLV68ONyjRZThw4dr9uzZysrKUt++ffXOO+/o4Ycf1l133RXu0cLHRalHHnnEZWVlufj4eDdw4EC3YcOGcI8UUSS1uixdujTco0W0q666yk2aNCncY0ScF1980fXr188lJCS47Oxst3jx4nCPFHF8Pp+bNGmSy8rKcomJia53797uvvvuc83NzeEeLWyi8nMwAIDIF3X3YAAA0YHAAABMEBgAgAkCAwAwQWAAACYIDADABIEBAJggMAAAEwQGAGCCwAAATBAYAICJ/wNl/d35EGuCdQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: down, Next State: (0, 1), Reward: 2, Total Reward: 4\n",
      "Action: right, Next State: (1, 1), Reward: 2, Total Reward: 6\n",
      "Action: right, Next State: (2, 1), Reward: 2, Total Reward: 8\n",
      "Action: up, Next State: (2, 0), Reward: 2, Total Reward: 10\n",
      "Action: right, Next State: (3, 0), Reward: 2, Total Reward: 12\n",
      "Action: left, Next State: (2, 0), Reward: 2, Total Reward: 14\n",
      "Action: up, Next State: (2, 0), Reward: 2, Total Reward: 16\n",
      "Action: down, Next State: (2, 1), Reward: 2, Total Reward: 18\n",
      "Action: up, Next State: (2, 0), Reward: 2, Total Reward: 20\n",
      "Action: down, Next State: (2, 1), Reward: 2, Total Reward: 22\n",
      "Action: up, Next State: (2, 0), Reward: 2, Total Reward: 24\n",
      "Action: up, Next State: (2, 0), Reward: 2, Total Reward: 26\n",
      "Action: down, Next State: (2, 1), Reward: 2, Total Reward: 28\n",
      "Action: up, Next State: (2, 0), Reward: 2, Total Reward: 30\n",
      "Action: right, Next State: (3, 0), Reward: 2, Total Reward: 32\n",
      "Action: left, Next State: (2, 0), Reward: 2, Total Reward: 34\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(world, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Run the agent for a specified number of steps\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Plot the path taken by the agent\u001b[39;00m\n\u001b[1;32m     53\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot_path()\n",
      "Cell \u001b[0;32mIn[45], line 32\u001b[0m, in \u001b[0;36mAgent.run\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m     30\u001b[0m     ax\u001b[38;5;241m.\u001b[39mimshow(grid \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreys\u001b[39m\u001b[38;5;124m'\u001b[39m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgent Path\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpause\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m plt\u001b[38;5;241m.\u001b[39mioff()\n",
      "File \u001b[0;32m~/Documents/Civilingenioer_Robotteknologi/Bachelor/src/.venv/lib/python3.10/site-packages/matplotlib/pyplot.py:760\u001b[0m, in \u001b[0;36mpause\u001b[0;34m(interval)\u001b[0m\n\u001b[1;32m    758\u001b[0m     canvas\u001b[38;5;241m.\u001b[39mstart_event_loop(interval)\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 760\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Agent:\n",
    "    def __init__(self, world, start_state):\n",
    "        self.world = world\n",
    "        self.current_state = start_state\n",
    "        self.total_reward = 0\n",
    "        self.path = [start_state]\n",
    "\n",
    "    def choose_action(self):\n",
    "        return np.random.choice(ACTIONS)\n",
    "\n",
    "    def take_action(self, action):\n",
    "        next_state = self.world.get_next_state(self.current_state, action)\n",
    "        reward = self.world.get_reward(*next_state)\n",
    "        self.total_reward += reward\n",
    "        self.current_state = next_state\n",
    "        self.path.append(next_state)\n",
    "        return next_state, reward\n",
    "\n",
    "    def run(self, steps):\n",
    "        plt.ion()\n",
    "        fig, ax = plt.subplots()\n",
    "        for _ in range(steps):\n",
    "            action = self.choose_action()\n",
    "            next_state, reward = self.take_action(action)\n",
    "            print(f\"Action: {action}, Next State: {next_state}, Reward: {reward}, Total Reward: {self.total_reward}\")\n",
    "            ax.clear()\n",
    "            grid = np.copy(self.world.grid)\n",
    "            for x, y in self.path:\n",
    "                grid[x, y] = 'A'\n",
    "            ax.imshow(grid == 'A', cmap='Greys', interpolation='nearest')\n",
    "            ax.set_title('Agent Path')\n",
    "            plt.pause(0.5)\n",
    "        plt.ioff()\n",
    "\n",
    "    def plot_path(self):\n",
    "        grid = np.copy(self.world.grid)\n",
    "        for x, y in self.path:\n",
    "            grid[x, y] = 'A'\n",
    "        plt.imshow(grid == 'A', cmap='Greys', interpolation='nearest')\n",
    "        plt.title('Agent Path')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the agent at a starting position (0, 0)\n",
    "agent = Agent(world, (0, 0))\n",
    "\n",
    "# Initialize the agent at a starting position (0, 0)\n",
    "agent = Agent(world, (0, 0))\n",
    "\n",
    "# Run the agent for a specified number of steps\n",
    "agent.run(50)\n",
    "\n",
    "# Plot the path taken by the agent\n",
    "agent.plot_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to linear need to be at least 1D, but they are 0D and 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 125\u001b[0m\n\u001b[1;32m    122\u001b[0m world \u001b[38;5;241m=\u001b[39m World(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    124\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(world, state_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, action_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(ACTIONS))\n\u001b[0;32m--> 125\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 107\u001b[0m, in \u001b[0;36mDQNAgent.train_agent\u001b[0;34m(self, episodes, update_target_every)\u001b[0m\n\u001b[1;32m    105\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 107\u001b[0m     action, next_state, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m update_target_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[55], line 94\u001b[0m, in \u001b[0;36mDQNAgent.take_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Takes action in the world, updates state, and stores experience.\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mget_state_representation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_state)\n\u001b[0;32m---> 94\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_experience(state, action, reward, next_state, done)\n",
      "Cell \u001b[0;32mIn[55], line 50\u001b[0m, in \u001b[0;36mDQNAgent.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     48\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 50\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ACTIONS[torch\u001b[38;5;241m.\u001b[39margmax(q_values)\u001b[38;5;241m.\u001b[39mitem()]\n",
      "File \u001b[0;32m~/Documents/Civilingenioer_Robotteknologi/Bachelor/src/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Civilingenioer_Robotteknologi/Bachelor/src/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[55], line 19\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/Documents/Civilingenioer_Robotteknologi/Bachelor/src/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Civilingenioer_Robotteknologi/Bachelor/src/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Civilingenioer_Robotteknologi/Bachelor/src/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to linear need to be at least 1D, but they are 0D and 2D"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "ACTIONS = [\"up\", \"down\", \"left\", \"right\"]  # Discrete action space\n",
    "\n",
    "# Neural Network for Q-learning\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for all actions\n",
    "\n",
    "# Deep Q-Learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, world, state_size, action_size, gamma=0.99, alpha=0.001, epsilon=0.1):\n",
    "        self.world = world\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        \n",
    "        self.memory = deque(maxlen=5000)  # Experience replay memory\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Sync target network\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=alpha)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(ACTIONS)  # Explore\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.model(state_tensor)\n",
    "            return ACTIONS[torch.argmax(q_values).item()]  # Exploit\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experiences for replay.\"\"\"\n",
    "        action_index = ACTIONS.index(action)\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model using experience replay.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return  # Not enough data to train\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        next_states_tensor = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Compute Q values\n",
    "        q_values = self.model(states_tensor)\n",
    "        q_values = q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute target Q values\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = self.target_model(next_states_tensor).max(1)[0]\n",
    "            target_q_values = rewards_tensor + (1 - dones_tensor) * self.gamma * max_next_q_values\n",
    "\n",
    "        # Compute loss and update model\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network with current model weights.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def take_action(self):\n",
    "        \"\"\"Takes action in the world, updates state, and stores experience.\"\"\"\n",
    "        state = self.world.get_state_representation(self.current_state)\n",
    "        action = self.choose_action(state)\n",
    "        next_state, reward, done = self.world.step(action)\n",
    "\n",
    "        self.store_experience(state, action, reward, next_state, done)\n",
    "        self.current_state = next_state\n",
    "        return action, next_state, reward\n",
    "\n",
    "    def train_agent(self, episodes, update_target_every=10):\n",
    "        \"\"\"Train the agent in multiple episodes.\"\"\"\n",
    "        for episode in range(episodes):\n",
    "            self.current_state = self.world.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, next_state, reward = self.take_action()\n",
    "                self.train()\n",
    "            if episode % update_target_every == 0:\n",
    "                self.update_target_network()\n",
    "\n",
    "    def run(self, steps):\n",
    "        \"\"\"Run the trained agent in real-time.\"\"\"\n",
    "        self.current_state = self.world.reset()\n",
    "        for _ in range(steps):\n",
    "            action, next_state, reward = self.take_action()\n",
    "            print(f\"Action: {action}, Next State: {next_state}, Reward: {reward}\")\n",
    "\n",
    "\n",
    "# Initialize the world and agent\n",
    "\n",
    "world = World(10, 10)\n",
    "world.add_reward(2, 3, \"+\")\n",
    "world.add_reward(1, 2, \"+\")\n",
    "world.add_reward(3, 2, \"+\")\n",
    "world.add_reward(4, 3, \"+\")\n",
    "world.add_reward(9, 3, \"+\")\n",
    "world.add_reward(9, 9, \"+\")\n",
    "world.add_reward(7, 8, \"+\")\n",
    "\n",
    "agent = DQNAgent(world, state_size=4, action_size=len(ACTIONS))\n",
    "agent.train_agent(episodes=1000)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
